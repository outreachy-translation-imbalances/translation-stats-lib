#!/usr/bin/env python3
"""
Extract Babel extension language proficiencies directly from the database tables
generated by the Babel parser function.

Ported to analytics cluster from translation_stats/language_proficiency_babel_extension.py

Should be run from the analytics conda environment.
"""

import os
import wmfdata.mariadb
import wmfdata.spark
import wmfdata.utils


database = os.getenv('DATABASE')


def fetch_babel_data(dbs):
    # TODO: Reuse the results of content_translation_revisions.
    return wmfdata.mariadb.run(
        """
        SELECT
            database() as wiki,
            actor_name AS user_name,
            babel_lang as language_used,
            babel_level as language_level
        FROM actor
        JOIN babel ON babel.babel_user = actor.actor_user
        """,
        dbs
    )


spark = wmfdata.spark.create_session()
dbs = wmfdata.utils.get_dblist("wikipedia")
result = fetch_babel_data(dbs)
spark.createDataFrame(result)\
    .write.saveAsTable(f"{database}.babel_extension_user_languages")

# 462,740 rows
